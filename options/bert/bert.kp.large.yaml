exp:
  resume: null
  outpath: logs/f30k_precomp/kp-bert-large-freeze9/
dataset:
  text_repr: word
  loader_name: precomp
  tokenizer_name: bert
  tokenizer_params: 
    model: bert-large-uncased
  vocab_paths: [bert]
  train:
    data: f30k_precomp.en
    workers: 8
    batch_size: 100
  val:
    data: [f30k_precomp.en]
    workers: 8
    batch_size: 256
    limit: 5000
model:
  latent_size: 1024
  # freeze_modules: []
  freeze_modules: [
    "model.txt_enc.bert.module.embeddings",
    "model.txt_enc.bert.module.encoder.layer[0]",
    "model.txt_enc.bert.module.encoder.layer[1]",    
    "model.txt_enc.bert.module.encoder.layer[2]",
    "model.txt_enc.bert.module.encoder.layer[3]",
    "model.txt_enc.bert.module.encoder.layer[4]",
    "model.txt_enc.bert.module.encoder.layer[5]",
    "model.txt_enc.bert.module.encoder.layer[6]",
    "model.txt_enc.bert.module.encoder.layer[7]",
    "model.txt_enc.bert.module.encoder.layer[8]",
    "model.txt_enc.bert.module.encoder.layer[9]",
    # "model.txt_enc.bert.module.encoder.layer[10]",
    # "model.txt_enc.bert.module.encoder.layer[11]",
    # "model.txt_enc.bert.module.encoder.layer[12]",
    # "model.txt_enc.bert.module.encoder.layer[13]",
    # "model.txt_enc.bert.module.encoder.layer[14]",    
  ]
  txt_enc:
    name: bert
    params: 
      model: bert-large-uncased
      feat_size: 1024
      use_gru: false
      word_level: false      
    pooling: none
    devices: cuda
  img_enc:
    name: simple
    params:
      img_dim: 2048
    devices:
    - cuda
    pooling: none
  similarity:
      name: kp_t2i
      params:
        latent_size: 1024
        reduce_proj: 8
        groups: 1024
        img_dim: 2048
        kernel_size: 1
        padding: 0
        norm_output: true
        gamma: 1
        train_gamma: true
        batchnorm: true
        device: cuda
  criterion:
    name: contrastive
    params:
      margin: 0.2
      max_violation: false
      weight: 1.0
      beta: 0.991
engine:
  eval_before_training: false
  debug: false
  print_freq: 50
  nb_epochs: 100
  early_stop: 50
  valid_interval: 500
  log_grad_norm: true
misc:
  cuda: true
  distributed: false
  seed: 1337
optimizer:
  # name: adamax
  # params:
  #   lr: 0.001 # 7e-4
  #   gradual_warmup_steps: [0.5, 1.0, 4000] #torch.linspace
  #   lr_decay_epochs: [10000, 20000, 3000] #range
  #   lr_decay_rate: .25  
  # lr_groups: model.txt_enc.bert
  # lr_groups_mult:  0.1
# optimizer:
  name: adam
  # import: lavse.optimizers.factory
  params:
    lr: 0.0002
  lr_scheduler:
    name: step
    params:
      step_size: 15000
      gamma: 0.1
  grad_clip: 1.
  lr_groups: model.txt_enc.bert
  lr_groups_mult:  0.00002
  # warmup: 
  #   multiplier: 8
  #   total_epoch: 10
  #   after_scheduler=scheduler_cosine
    
