exp:
  outpath: runs/temp/hier_900_softmax_glove_smooth25_lr/
  resume: null # last, best_[...], or empty (from scratch)
dataset:  
  vocab_paths: [.vocab_cache/f30k_precomp.json]
  text_repr: word
  loader_name: precomp
  train:
    data: f30k_precomp.en
    workers: 2
    batch_size: 900
  val: 
    data: [f30k_precomp.en]
    workers: 1
    batch_size: 32
    limit: 5000
model:
  latent_size: 1024
  freeze_modules: []
  txt_enc:
    name: gru_glove
    params:
      embed_dim: 300
      use_bi_gru: true
      glove_path: '.vocab_cache/glove_840B_f30k_precomp.json.pkl'
      add_rand_embed: True
    pooling: lens
    devices: [cuda,]
  img_enc:
    name: hierarchical
    params:
      img_dim: 2048
    devices: [cuda,]
    pooling: mean
  similarity:
    name: cosine
    params:
      device: cuda
    device: cuda    
  # similarity:
  #   name: adapt_i2t
  #   params:
  #     latent_size: 1024
  #     # k: 8
  #     norm: False
  #     device: cuda # FIXME
  #   device: cuda # FIXME
criterion:
  name: softmax
  params:
    max_violation: False
    smooth: 25
optimizer:
  name: adamax
  params:
    lr: 0.001 # 7e-4
    gradual_warmup_steps: [0.5, 2.0, 600] #torch.linspace
    lr_decay_epochs: [2500, 20000, 400] #range
    lr_decay_rate: .25
  lr_scheduler: 
    name: null
    params:
      step_size: 250
      gamma: 1      
  grad_clip: 2.
engine:
  eval_before_training: False
  debug: False
  print_freq: 20
  nb_epochs: 100
  early_stop: 50
  valid_interval: 75
misc: # TODO
  cuda: True
  distributed: False # TODO 
  seed: 1337 # TODO
