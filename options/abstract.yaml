exp:
  outpath: logs/vqa2/debugs
  resume: null # last, best_[...], or empty (from scratch)
dataset:  
  vocab_path: vocab/complete.json
  text_repr: word
  loader_name: precomp
  train:
    data: f30k_precomp.en
    workers: 1
    batch_size: 128
  val: 
    data: [f30k_precomp.en]
    workers: 1
    batch_size: 32
    limit: 5000
model:
  latent_size: 1024
  freeze_modules: [] # TODO: implement
  # freeze_modules: [txt_enc.embedding, img_enc.cnn]  
  txt_enc:
    name: gru
    params:
      embed_dim: 300
      use_bi_gru: true
    pooling: lens
    devices: [cuda,]
  img_enc:
    name: vsepp
    params:
      img_dim: 2048
    devices: [cuda,]
    pooling: mean
  similarity:
    name: cosine
    params:
      device: cuda
    device: cuda
criterion:
  margin: 0.2
  max_violation: False
  beta: 0.991
optimizer:
  import: lavse.optimizers.factory
  name: Adam
  lr: 0.0007
  lr_scheduler: 
    name: 'step'
    params:
      step_size: 10
      gamma: 0.1
      # gradual_warmup_steps: [0.5, 2.0, 4] #torch.linspace
      # lr_decay_epochs: [10, 20, 2] #range
      # lr_decay_rate: .25
  grad_clip: 2.
engine:
  name: logger
  eval_before_training: False
  debug: False
  print_freq: 10
  nb_epochs: 22
  early_stop: 50
  valid_interval: 500
misc: # TODO
  cuda: True
  distributed: False # TODO 
  seed: 1337
