exp:
  resume: null # last, best_[...], or empty (from scratch)
dataset:  
  vocab_paths: [.vocab_cache/char.json,]
  text_repr: liwe
  loader_name: precomp
  train:
    workers: 1
    batch_size: 128
  val:
    workers: 1
    batch_size: 64
  adapt:
    workers: 1
    batch_size: 128
model:
  latent_size: 1024
  freeze_modules: []
  txt_enc:
    name: liwe_gru
    params:
      embed_dim: 300
      liwe_neurons: [384, 448]
      liwe_activation: nn.LeakyReLU(0.1, inplace=True)
      use_bi_gru: true
    pooling: lens    
  img_enc:
    name: simple
    params:
      img_dim: 2048    
    pooling: mean    
  similarity:
    name: cosine
  ml_similarity: 
    name: cosine
  criterion:
    name: 'contrastive'
    params: 
      margin: 0.2
      max_violation: False
      beta: 0.991
  ml_criterion:
    name: 'contrastive'
    params:
      margin: 0.2
      max_violation: False
      beta: 0.91
engine:
  eval_before_training: False
  debug: False
  print_freq: 10
  nb_epochs: 30
  early_stop: 50
  valid_interval: 500
misc: # TODO
  cuda: True
  distributed: False # TODO 
  seed: 1337 # TODO
