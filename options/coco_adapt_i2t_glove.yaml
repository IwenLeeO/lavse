__include__: abstract.yaml
exp:
  outpath: logs/coco_precomp/adapt_i2t/glove+rand-freeze-simple-lr7e-4/
  resume: null # last, best_[...], or empty (from scratch)
dataset:  
  vocab_paths: [.vocab_cache/coco_precomp.json]
  text_repr: word
  loader_name: precomp
  train:
    data: coco_precomp.en
    workers: 1
    batch_size: 128
  val: 
    data: [coco_precomp.en]
    workers: 1
    batch_size: 32
    limit: 5000
model:
  latent_size: 1024
  sfreeze_modules: [model.txt_enc.embed.glove,]
  freeze_modules: []
  txt_enc:
    name: gru_glove
    params:
      embed_dim: 300
      use_bi_gru: true
      glove_path: '.vocab_cache/glove_840B_coco_precomp.json.pkl'
      add_rand_embed: true
    pooling: none
    devices: [cuda,]
  img_enc:
    name: simple
    params:
      img_dim: 2048
    devices: [cuda,]
    pooling: none
  similarity:
    name: adapt_i2t
    params:
      latent_size: 1024
      k: 1
      norm: False
      cond_vec: False
      device: cuda # FIXME
    device: cuda # FIXME
optimizer:
  name: adamax
  params:
    lr: 0.0007
    gradual_warmup_steps: [0.5, 2.0, 16000] #torch.linspace
    lr_decay_epochs: [40000, 80000, 8000] #range 
    lr_decay_rate: .25
  lr_scheduler: 
    name: null  


  # lr: 0.0007
  # gradual_warmup_steps: [0.5, 2.0, 4] #torch.linspace
  # lr_decay_epochs: [10, 20, 2] #range
  # lr_decay_rate: .25
