exp:
  resume: null
  outpath: logs/f30k_precomp.en/clmr/
dataset:
  vocab_paths:
  - .vocab_cache/complete_precomp.json
  text_repr: word
  loader_name: precomp
  train:
    workers: 1
    batch_size: 128
    data: f30k_precomp.en
  val:
    workers: 1
    batch_size: 64
    data:
    - f30k_precomp.en
  adapt:
    workers: 1
    batch_size: 128
    data: []
model:
  latent_size: 1024
  freeze_modules: []
  txt_enc:
    name: gru
    params:
      embed_dim: 300
      use_bi_gru: true
    pooling: lens    
  img_enc:
    name: hierarchical
    params:
      img_dim: 2048    
    pooling: mean
  similarity:
    name: cosine
  criterion:
    name: contrastive
    params:
      margin: 0.2
      max_violation: false
      beta: 0.991
optimizer:
  name: adam
  import: lavse.optimizers.factory
  params:
    lr: 0.0006
  lr_scheduler:
    name: step
    params:
      step_size: 15000
      gamma: 0.1
  grad_clip: 2.0
engine:
  eval_before_training: false
  debug: false
  print_freq: 10
  nb_epochs: 30
  early_stop: 50
  valid_interval: 500
misc:
  sync_bn: false
  cuda: true
  distributed: True
  seed: 1337
