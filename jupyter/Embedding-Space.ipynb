{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "\n",
    "import params\n",
    "from lavse.data.loaders import get_loader\n",
    "from lavse.model import model\n",
    "from lavse.train.train import Trainer\n",
    "from lavse.utils import file_utils, helper\n",
    "from lavse.utils.logger import create_logger\n",
    "from run import load_yaml_opts, parse_loader_name\n",
    "from tqdm import tqdm\n",
    "from lavse.data.collate_fns import Collate\n",
    "from lavse.utils import layers\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lavse.train.evaluation import predict_loader\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# matplotlib.rc('font', family='fonts-japanese-mincho')\n",
    "# matplotlib.rc('font', family='Arial')\n",
    "# matplotlib.rcParams['font.fantasy'] = 'Ubuntu'\n",
    "from torchvision import transforms\n",
    "collate = Collate()\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/opt/jonatas/repos/lavse/logs_aaai2/f30k_precomp/adapt_t2i/glove-freeze-img_norm_softmax/')\n",
    "opt = load_yaml_opts(path / 'options.yaml')\n",
    "model_path = path / 'best_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt['model']['txt_enc']['params']['glove_path'] = '/opt/jonatas/repos/lavse/.vocab_cache/glove_f30k_precomp.json.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'gru_glove',\n",
       " 'params': {'embed_dim': 300,\n",
       "  'use_bi_gru': True,\n",
       "  'glove_path': '/opt/jonatas/repos/lavse/.vocab_cache/glove_f30k_precomp.json.pkl',\n",
       "  'add_rand_embed': True},\n",
       " 'pooling': 'none',\n",
       " 'devices': ['cuda']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt['model']['txt_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "f30k_precomp\n",
      "Image div 1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ngpu = 1\n",
    "loaders = []\n",
    "\n",
    "if 'DATA_PATH' not in os.environ:\n",
    "    data_path = '/opt/jonatas/datasets/lavse'\n",
    "#     data_path = '../lavse'\n",
    "#     data_path = opt.dataset.data_path\n",
    "else:\n",
    "    data_path = os.environ['DATA_PATH']\n",
    "\n",
    "\n",
    "print(opt.dataset.lang)\n",
    "opt.dataset.vocab_paths = ['../{}'.format(p) for p in opt.dataset.vocab_paths]\n",
    "for data_name in opt.dataset.val.data:\n",
    "    data_name, lang = parse_loader_name(data_name)\n",
    "    print(data_name)\n",
    "    loader = get_loader(\n",
    "        data_split='test',\n",
    "        data_path=data_path,\n",
    "        data_name=data_name,\n",
    "        loader_name=opt.dataset.loader_name,\n",
    "        local_rank=0,\n",
    "        lang=lang,\n",
    "        text_repr=opt.dataset.text_repr,\n",
    "        vocab_paths=opt.dataset.vocab_paths,\n",
    "        ngpu=ngpu,\n",
    "        **opt.dataset.val,\n",
    "    )\n",
    "    loaders.append(loader)\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "print(len(loaders))\n",
    "loader = loaders[0]\n",
    "tokenizers = loader.dataset.tokenizers\n",
    "if type(tokenizers) != list:\n",
    "    tokenizers = [tokenizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and options\n",
    "model = model.LAVSE(**opt.model, tokenizers=tokenizers)#.to(device)\n",
    "checkpoint = helper.restore_checkpoint(\n",
    "    path=model_path,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "model.set_devices_(\n",
    "    txt_devices=[device],\n",
    "    img_devices=[device],\n",
    "    loss_device=device,\n",
    ")\n",
    "is_master = True\n",
    "model.master = is_master # FIXME: Replace \"if print\" by built_in print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavse.model.similarity.measure import cosine_sim, l2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 36, 1024) (5000, 77, 1024)\n"
     ]
    }
   ],
   "source": [
    "img_embs, cap_embs, cap_lens = predict_loader(model, loader, device)\n",
    "print(img_embs.shape, cap_embs.shape)\n",
    "# img_embs /= np.linalg.norm(img_embs, axis=1, keepdims=True)\n",
    "# cap_embs /= np.linalg.norm(cap_embs, axis=1, keepdims=True)\n",
    "# print(img_embs.shape, cap_embs.shape)\n",
    "# cap_embs = l2norm(torch.tensor(cap_embs).float(), -1)\n",
    "# img_embs = l2norm(torch.tensor(img_embs).mean(1).float(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling_with_len(x, lens):\n",
    "    max_len = int(np.max(lens))\n",
    "    mask = torch.arange(max_len).expand(len(lens), max_len) < torch.tensor(lens).long().unsqueeze(1)\n",
    "    mask = mask.float().cpu()\n",
    "    vector = x.sum(1) / mask.cpu().unsqueeze(2).sum(1)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = mean_pooling_with_len(torch.tensor(cap_embs).float().cpu(), cap_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cemb = l2norm(x, -1)\n",
    "iemb = l2norm(torch.tensor(img_embs).mean(1).float(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cemb.shape, iemb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(perplexity=40,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data = np.concatenate([cemb[:500], iemb[:100]], 0)\n",
    "print(fit_data.shape)\n",
    "data = tsne.fit_transform(fit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set(style='white', )\n",
    "colors = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = np.arange(0, 600, 1) // 5\n",
    "c = [colors[i % len(colors)] for i in ci]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0]\n",
    "y = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=c, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.8286e-02, 5.7221e-02, 3.7450e-02,  ..., 2.0226e-02,\n",
      "          9.1577e-02, 9.1944e-02],\n",
      "         [7.6146e-03, 2.3379e-03, 6.0560e-03,  ..., 3.5624e-03,\n",
      "          6.7657e-03, 4.6830e-03],\n",
      "         [2.8352e-01, 1.3711e-02, 9.2421e-03,  ..., 2.9285e-03,\n",
      "          2.1840e-04, 4.3831e-08],\n",
      "         ...,\n",
      "         [3.0719e-05, 3.6107e-02, 7.4170e-04,  ..., 2.7239e-05,\n",
      "          6.0069e-05, 5.2305e-03],\n",
      "         [8.3180e-03, 2.1354e-01, 4.7016e-02,  ..., 4.7950e-03,\n",
      "          2.4076e-02, 3.8247e-03],\n",
      "         [4.6009e-03, 1.7261e-02, 4.0310e-03,  ..., 1.2766e-02,\n",
      "          4.8167e-04, 5.2097e-05]],\n",
      "\n",
      "        [[3.3337e-04, 9.8366e-05, 2.1892e-04,  ..., 3.7477e-04,\n",
      "          1.1196e-04, 4.5753e-04],\n",
      "         [7.1099e-07, 9.4101e-04, 1.5661e-06,  ..., 4.9198e-04,\n",
      "          2.2967e-06, 1.9031e-04],\n",
      "         [1.2938e-03, 1.8615e-02, 1.0400e-02,  ..., 1.0214e-01,\n",
      "          1.4984e-03, 9.1611e-02],\n",
      "         ...,\n",
      "         [4.2468e-04, 3.5940e-02, 5.4845e-04,  ..., 5.1939e-04,\n",
      "          8.6347e-03, 4.9577e-03],\n",
      "         [9.9959e-01, 4.7838e-08, 1.4430e-07,  ..., 1.3821e-07,\n",
      "          2.0851e-09, 2.0012e-07],\n",
      "         [1.9626e-12, 4.3211e-14, 1.6566e-13,  ..., 1.7461e-13,\n",
      "          6.3682e-14, 2.5708e-13]],\n",
      "\n",
      "        [[6.1572e-07, 3.3042e-07, 1.8495e-06,  ..., 9.3811e-07,\n",
      "          3.7883e-06, 3.4373e-07],\n",
      "         [2.3197e-03, 8.7877e-04, 9.5915e-04,  ..., 1.5805e-03,\n",
      "          7.2016e-04, 1.0453e-03],\n",
      "         [7.2253e-06, 5.8594e-06, 2.8853e-06,  ..., 7.3714e-06,\n",
      "          4.0711e-05, 8.2426e-05],\n",
      "         ...,\n",
      "         [9.1941e-05, 1.7937e-03, 8.3848e-05,  ..., 6.9638e-05,\n",
      "          8.7295e-05, 3.1486e-01],\n",
      "         [1.1745e-03, 3.7912e-04, 1.4840e-03,  ..., 2.6981e-03,\n",
      "          3.1568e-01, 1.4699e-01],\n",
      "         [2.7041e-02, 2.3338e-02, 2.1041e-01,  ..., 2.9491e-02,\n",
      "          1.8407e-03, 2.9967e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.4197e-04, 4.5076e-04, 8.3570e-04,  ..., 3.7079e-04,\n",
      "          4.4448e-04, 3.3028e-04],\n",
      "         [8.0284e-07, 6.2380e-08, 1.9983e-06,  ..., 4.6776e-08,\n",
      "          1.2921e-06, 1.0126e-05],\n",
      "         [2.9112e-02, 2.7177e-02, 9.8802e-02,  ..., 5.0691e-02,\n",
      "          2.9181e-01, 2.6763e-06],\n",
      "         ...,\n",
      "         [9.6557e-03, 1.2691e-02, 1.2991e-02,  ..., 1.2559e-02,\n",
      "          9.7011e-03, 1.5861e-02],\n",
      "         [2.3825e-03, 5.2348e-03, 5.4755e-03,  ..., 3.8698e-03,\n",
      "          3.6251e-03, 6.3999e-05],\n",
      "         [3.5470e-03, 5.7216e-02, 1.4028e-01,  ..., 6.6459e-02,\n",
      "          2.5370e-02, 3.8378e-04]],\n",
      "\n",
      "        [[2.1809e-02, 1.7344e-02, 1.9424e-02,  ..., 1.4606e-02,\n",
      "          1.7003e-02, 1.9947e-02],\n",
      "         [7.5531e-02, 5.9746e-07, 7.3938e-03,  ..., 3.3170e-06,\n",
      "          3.4639e-02, 2.0314e-02],\n",
      "         [8.3818e-04, 1.3631e-05, 2.3297e-04,  ..., 9.7502e-05,\n",
      "          1.0967e-04, 1.9342e-04],\n",
      "         ...,\n",
      "         [1.4044e-02, 2.7138e-02, 2.8210e-02,  ..., 4.2247e-02,\n",
      "          1.9816e-02, 3.0831e-02],\n",
      "         [1.3518e-02, 1.3067e-02, 1.5140e-02,  ..., 4.7128e-02,\n",
      "          1.6188e-03, 1.0552e-02],\n",
      "         [2.6390e-02, 4.0512e-04, 2.9134e-02,  ..., 1.3751e-02,\n",
      "          2.7681e-02, 4.0790e-02]],\n",
      "\n",
      "        [[1.2365e-11, 9.8190e-11, 4.2380e-11,  ..., 2.4472e-10,\n",
      "          2.5369e-10, 6.9428e-10],\n",
      "         [2.3504e-05, 4.6994e-06, 4.9974e-06,  ..., 8.4850e-08,\n",
      "          1.6688e-05, 3.7129e-07],\n",
      "         [1.5236e-06, 5.6702e-03, 2.0219e-03,  ..., 6.7792e-10,\n",
      "          6.4511e-03, 2.1954e-05],\n",
      "         ...,\n",
      "         [2.4302e-03, 1.1032e-03, 3.2551e-03,  ..., 8.3546e-01,\n",
      "          5.6794e-03, 4.3082e-03],\n",
      "         [4.6954e-03, 1.0285e-02, 1.4162e-02,  ..., 6.7931e-03,\n",
      "          5.0822e-03, 4.4683e-04],\n",
      "         [7.5824e-38, 3.8211e-37, 3.1992e-37,  ..., 2.4715e-37,\n",
      "          1.5850e-37, 3.8169e-37]]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "_ = model.similarity.similarity(\n",
    "    torch.tensor(img_embs[:10]).float().cuda(), \n",
    "    torch.tensor(cap_embs[:50]).float().cuda(),\n",
    "    cap_lens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.3943, 50.3701, 42.1236, 24.6865, 16.4362, 27.0679, 25.6940, 30.5770,\n",
       "        12.3891, 24.7710, 18.2719, 26.3849, 40.6504, 25.8636, 36.9654, 36.6643,\n",
       "        28.6649, 23.9550, 58.5975, 38.1512, 23.2272, 41.8049, 32.1977, 20.3253,\n",
       "        17.3744, 20.6523, 22.2894, 28.1067, 24.7474, 30.7673, 27.4584, 13.4958,\n",
       "        15.8274, 18.3241, 29.1415, 28.5817], device='cuda:0',\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.load('/opt/jonatas/datasets/lavse/jap_precomp/test_ids.npy')\n",
    "new_ids = np.zeros((len(ids)*5), dtype=np.int64\n",
    "for i in range(len(ids)):\n",
    "    new_ids[i*5:(i*5)+5] = ids[i]\n",
    "    \n",
    "loader.dataset.ids = new_ids\n",
    "print(ids.min(), ids.max(), ids.mean())\n",
    "\n",
    "jp_caps = np.loadtxt('/opt/jonatas/datasets/lavse/jap_precomp/test_caps.jp.txt', delimiter='@', dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/opt/jonatas/datasets/lavse/f30k/dataset_flickr30k.json', 'r') as fp:\n",
    "#     flickr = json.load(fp)\n",
    "#     f30k = {x['filename']: x for x in flickr['images']}\n",
    "with open('/opt/jonatas/datasets/lavse/jap_precomp/yjcaptions26k/yjcaptions26k_clean.json', 'r') as fp:\n",
    "    flickr = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_info = {x['id']: x for x in flickr['images']}\n",
    "\n",
    "flickr['images'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs, cap_embs, cap_lens = torch.tensor(img_embs), torch.tensor(cap_embs), torch.tensor(cap_lens)\n",
    "sims = model.get_sim_matrix_shared(\n",
    "        embed_a=img_embs, embed_b=cap_embs,\n",
    "        lens=cap_lens, shared_size=128\n",
    "    )\n",
    "sims = layers.tensor_to_numpy(sims)\n",
    "print(sims.shape)\n",
    "new_sims = np.zeros((4885, 4885))\n",
    "for i in range(977):\n",
    "    new_sims[i*5:(i*5)+5] = sims[i]\n",
    "sims = new_sims\n",
    "print(sims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims.min(), sims.max(), sims.mean())\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flickr['images'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT2IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 2 img\n",
    "lang = 'jp'\n",
    "sims_t = sims.T\n",
    "sims_t1k = sims_t[:, range(0, sims_t.shape[1], 5)]\n",
    "print('sim1k', sims_t1k.shape)\n",
    "inds = np.argsort(sims_t1k, axis=1)[:, ::-1]\n",
    "\n",
    "ld = loader\n",
    "\n",
    "cap_idx = 51\n",
    "\n",
    "for cap_idx in np.random.choice(4885, 100, replace=False):\n",
    "#     cap = ld.dataset.captions[cap_idx]\n",
    "    cap = jp_caps[cap_idx]\n",
    "    print(cap)\n",
    "\n",
    "    n = 15\n",
    "    if cap.endswith('.'):\n",
    "        cap = cap[:-2]\n",
    "    split_str = cap.split(' ')\n",
    "    if len(split_str) > n:\n",
    "        for j in range(n, len(split_str), n):\n",
    "            split_str[j-1] += '\\n   '\n",
    "    cap = ' '.join(split_str)\n",
    "    cap = '{}: {}'.format('Query', cap)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.gca().set_axis_off()\n",
    "    plt.margins(0,0)\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(bottom=0, top=1, left=0, right=1, wspace=0, hspace=0)\n",
    "\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    imgs = []\n",
    "    preds = []\n",
    "    for i in range(0, 3):\n",
    "        img_idx = inds[cap_idx, i] * 5\n",
    "        pred = cap_idx in range(img_idx, img_idx + 5)\n",
    "        preds.append(pred)\n",
    "\n",
    "        # restore original indexes\n",
    "        img_id = int(ld.dataset.ids[img_idx])\n",
    "        img_info = new_info[img_id]\n",
    "#         img_info = flickr['images'][img_id]\n",
    "        img_set = img_info['file_name'].split('_')[1]\n",
    "        img = Image.open(os.path.join('/opt/jonatas/datasets/lavse/coco/images', img_set, img_info['file_name']))\n",
    "        crop = transforms.CenterCrop(max(img.size))\n",
    "        img = crop(img).resize((400,400))\n",
    "        img = np.asarray(img)\n",
    "        if i < 2:\n",
    "            img = np.pad(img, [[0,0], [0,50], [0,0]], mode='constant', constant_values=255)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.hstack(imgs)\n",
    "    ax.imshow(imgs)\n",
    "    plt.text(imgs.shape[1]/2., -30, cap, fontsize=10, horizontalalignment='center')\n",
    "    first = 200\n",
    "    for i in range(3):\n",
    "        symb = '$\\checkmark$' if preds[i] else '$\\\\times$'\n",
    "        plt.text(first, img.shape[0] + 35, symb, fontsize=15, horizontalalignment='center')\n",
    "        first += 450\n",
    "    plt.savefig('/home/jonatas/retrieval_imgs/t2i/{}/t2i_{}_{}.pdf'.format(lang, lang, cap_idx), bbox_inches='tight', dpi=166, pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old code below\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# success cases\n",
    "lang = loader.dataset.lang\n",
    "print('sim matrix shape', sims.shape)\n",
    "inds = np.argsort(sims, axis=1)[:, ::-1]\n",
    "print('inds shape', inds.shape)\n",
    "from matplotlib.transforms import Bbox\n",
    "\n",
    "max_length = 60\n",
    "ld = loader\n",
    "for k in np.random.choice(np.arange(5, 5000, 5), 100, replace=False):#range(5, 5000, 5):\n",
    "    img_ix = k\n",
    "    img_id = int(ld.dataset.ids[img_ix])\n",
    "#     print 'img id', img_id\n",
    "    img_info = flickr['images'][img_id]\n",
    "    img = Image.open(os.path.join('/opt/jonatas/datasets/lavse/f30k/images/flickr30k_images', img_info['filename']))\n",
    "    crop = transforms.CenterCrop(max(img.size))\n",
    "    img = crop(img).resize((400,400))\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "#     fig.patch.set_visible(False)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(bottom=0, top=1, left=0, right=1, wspace=0, hspace=0)\n",
    "    \n",
    "    \n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(img)\n",
    "    gt = range(img_ix, img_ix + 5)\n",
    "\n",
    "    preds = []\n",
    "    caps = []\n",
    "    for i in range(5):\n",
    "        pred = inds[img_ix, i] in gt\n",
    "        preds.append(pred)\n",
    "        caps.append(ld.dataset.captions[inds[img_ix, i]])\n",
    "\n",
    "    n = 8\n",
    "    big_str = []\n",
    "    biggest_cap = 0\n",
    "    for i in range(5):\n",
    "        cap = caps[i]\n",
    "        if cap.endswith('.'):\n",
    "            cap = cap[:-2]\n",
    "        split_str = cap.split(' ')\n",
    "        if len(split_str) > n:\n",
    "            for j in range(n, len(split_str), n):\n",
    "                split_str[j-1] += '\\n   '\n",
    "        split_str = ' '.join(split_str)\n",
    "        symb = '$\\checkmark$' if preds[i] else '$\\\\times$'\n",
    "        br = ''\n",
    "        if i < 4:\n",
    "            br = '\\n\\n'\n",
    "        \n",
    "        new_cap = ''.join(split_str) + '.'\n",
    "        \n",
    "            \n",
    "        big_str.append('{}. {} {}{}'.format((i+1), new_cap, symb, br))\n",
    "    text = ''.join(big_str)\n",
    "    ax.text(img.width + 30, img.height/2, text, verticalalignment='center', fontsize=15)\n",
    "    \n",
    "#     if all(preds):\n",
    "#         print(\"all corect found :)\")\n",
    "    plt.savefig('/home/douglas/retrieval_imgs/i2t/random/{}/t2i_{}_{}.pdf'.format(lang, lang, img_ix), bbox_inches=Bbox([[0,0],[10,4]]), dpi=166, pad_inches=0)\n",
    "    if k % 500 == 0:\n",
    "        print('processed {} images'.format(k/5))\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure cases\n",
    "\n",
    "print 'sim matrix shape', sims.shape\n",
    "inds = np.argsort(sims, axis=1)[:, ::-1]\n",
    "print 'inds shape', inds.shape\n",
    "from matplotlib.transforms import Bbox\n",
    "\n",
    "max_length = 60\n",
    "\n",
    "ld = loader_de\n",
    "\n",
    "img_ix = 815\n",
    "img_id = int(ld.dataset.ids[img_ix])\n",
    "img_info = flickr['images'][img_id]\n",
    "img = Image.open(os.path.join('/opt/datasets/flickr/flickr30k_images', img_info['filename']))\n",
    "crop = transforms.CenterCrop(max(img.size))\n",
    "img = crop(img).resize((400,400))\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(bottom=0, top=1, left=0, right=1, wspace=0, hspace=0)\n",
    "\n",
    "\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "ax.imshow(img)\n",
    "gt = range(img_ix, img_ix + 5)\n",
    "\n",
    "preds = []\n",
    "caps = []\n",
    "for i in range(5):\n",
    "    pred = inds[img_ix, i] in gt\n",
    "    preds.append(pred)\n",
    "    caps.append(ld.dataset.captions[inds[img_ix, i]])\n",
    "\n",
    "n = 7\n",
    "big_str = []\n",
    "biggest_cap = 0\n",
    "for i in range(5):\n",
    "    cap = caps[i]\n",
    "    print cap\n",
    "    if cap.endswith('.'):\n",
    "        cap = cap[:-2]\n",
    "    split_str = cap.split(' ')\n",
    "    if len(split_str) > n:\n",
    "        for j in range(n, len(split_str), n):\n",
    "            split_str[j-1] += '\\n   '\n",
    "    split_str = ' '.join(split_str)\n",
    "    symb = '$\\checkmark$' if preds[i] else '$\\\\times$'\n",
    "    br = ''\n",
    "    if i < 4:\n",
    "        br = '\\n\\n'\n",
    "\n",
    "    new_cap = ''.join(split_str) + '.'\n",
    "    big_str.append('{}. {} {}{}'.format((i+1), new_cap, symb, br))\n",
    "\n",
    "ax.text(img.width + 30, img.height/2, unicode(''.join(big_str), 'utf-8'), verticalalignment='center', fontsize=15)\n",
    "\n",
    "plt.savefig('incorrect_predictions/i2t_de_{}_err.pdf'.format(img_ix), bbox_inches=Bbox([[0,0],[10,4]]), dpi=166, pad_inches=0)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 2 img\n",
    "\n",
    "sims_t = sims.T\n",
    "sims_t1k = sims_t[:, range(0, sims_t.shape[1], 5)]\n",
    "print 'sim1k', sims_t1k.shape\n",
    "inds = np.argsort(sims_t1k, axis=1)[:, ::-1]\n",
    "\n",
    "ld = loader_de\n",
    "\n",
    "cap_idx = 3100\n",
    "cap = ld.dataset.captions[cap_idx]\n",
    "print cap\n",
    "\n",
    "n = 10\n",
    "if cap.endswith('.'):\n",
    "    cap = cap[:-2]\n",
    "split_str = cap.split(' ')\n",
    "if len(split_str) > n:\n",
    "    for j in range(n, len(split_str), n):\n",
    "        split_str[j-1] += '\\n   '\n",
    "cap = ' '.join(split_str)\n",
    "cap = '{}: {}'.format('Query', cap)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.gca().set_axis_off()\n",
    "plt.margins(0,0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "plt.axis('off')\n",
    "plt.subplots_adjust(bottom=0, top=1, left=0, right=1, wspace=0, hspace=0)\n",
    "\n",
    "\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "imgs = []\n",
    "preds = []\n",
    "for i in range(0, 3):\n",
    "    img_idx = inds[cap_idx, i] * 5\n",
    "    pred = cap_idx in range(img_idx, img_idx + 5)\n",
    "    preds.append(pred)\n",
    "    \n",
    "    # restore original indexes\n",
    "    img_id = int(ld.dataset.ids[img_idx])\n",
    "    img_info = flickr['images'][img_id]\n",
    "    img = Image.open(os.path.join('/opt/datasets/flickr/flickr30k_images', img_info['filename']))\n",
    "    crop = transforms.CenterCrop(max(img.size))\n",
    "    img = crop(img).resize((400,400))\n",
    "    img = np.asarray(img)\n",
    "    if i < 2:\n",
    "        img = np.pad(img, [[0,0], [0,50], [0,0]], mode='constant', constant_values=255)\n",
    "    imgs.append(img)\n",
    "        \n",
    "imgs = np.hstack(imgs)\n",
    "ax.imshow(imgs)\n",
    "plt.text(imgs.shape[1]/2., -30, unicode(cap, 'utf-8'), fontsize=10, horizontalalignment='center')\n",
    "first = 200\n",
    "for i in range(3):\n",
    "    symb = '$\\checkmark$' if preds[i] else '$\\\\times$'\n",
    "    plt.text(first, img.shape[0] + 35, symb, fontsize=15, horizontalalignment='center')\n",
    "    first += 450\n",
    "plt.savefig('t2i_imgs/t2i_de_{}.pdf'.format(cap_idx), bbox_inches='tight', dpi=166, pad_inches=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
